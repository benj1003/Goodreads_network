{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "This notebook includes all the code used to scrape the information from Goodreads.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl #only necessary for Mac\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "context = ssl._create_unverified_context()  # only necessary for Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of must read books\n",
    "def scrape_table(url, max_page, context):\n",
    "  \"\"\"\n",
    "  Function scraping a url for table and returning dataframe with links to\n",
    "  the books mentioned in the table. \n",
    "  \n",
    "  Parameters:\n",
    "  url: link to page where table is located\n",
    "  max_page: how many pages of the table are wanted\n",
    "  context: only necessary for Mac in order to get the right ssl key\n",
    "  \n",
    "  Return:\n",
    "  Dataframe with all the links to the books as entries\n",
    "  \"\"\"\n",
    "  page = 1\n",
    "  all_titles = []\n",
    "  while page < max_page + 1:\n",
    "      targetUrl = url + str(page)\n",
    "      targetRequest = Request(targetUrl)\n",
    "      response = urlopen(targetRequest, context=context)\n",
    "      responseText = response.read()\n",
    "      response.close()\n",
    "      soup = BeautifulSoup(responseText, 'html.parser')\n",
    "      targetTable = soup.select(\"table\")[0]\n",
    "      a = targetTable.find_all('a', {'itemprop': 'url'})\n",
    "      tmp = re.findall(r'href=(\"(.*?)\")', str(a))\n",
    "      title = []\n",
    "      for i in range(len(tmp)):\n",
    "          if i % 2 == 0:\n",
    "              t = tmp[i][0]\n",
    "              title.append(t)\n",
    "      all_titles += title\n",
    "      print(f\"Done scraping page {page}\")\n",
    "      page += 1\n",
    "  all_titles_df = pd.DataFrame(all_titles, columns=['Book link'])\n",
    "  return all_titles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First at table with all the books on a wanted list is scraped, to later use as a look-up for gathering information about each book. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the top 2000 books from the list on goodreads\n",
    "\n",
    "url_good = \"https://www.goodreads.com/list/show/264.Books_That_Everyone_Should_Read_At_Least_Once?page=\"\n",
    "max_page = 20\n",
    "\n",
    "table_of_books = scrape_table(url_good, max_page, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_bad = \"https://www.goodreads.com/list/show/23974.Worst_Rated_Books_on_Goodreads?page=\"\n",
    "table_of_books_neg = scrape_table(url_bad, max_page, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next the information about each book must be retrieved. To do this two functions are defined. The first is used to retreive reviews from a given book and the second retreives all necessary information needed about the book and gathers it in a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get information from must read books\n",
    "\n",
    "def get_reviews(target_reviews, max_rev):\n",
    "    \"\"\"\n",
    "    The function gets a list of target reviews and opens individual pages for \n",
    "    the reviews and scrapes the raw text. \n",
    "    \n",
    "    Parameter:\n",
    "    target_reviews: Result set from soup.find_all with list of all review entries\n",
    "    max_rev: The target_reviews returns 30 reviews per book, and this parameters\n",
    "     allows a way to reduce how many reviews are wanted\n",
    "    \n",
    "    Return: \n",
    "    a list with each review as a raw text entry. \n",
    "    \"\"\"\n",
    "    reviews = []\n",
    "    if len(target_reviews) < max_rev:\n",
    "        max_rev = len(target_reviews)\n",
    "    for i in range(max_rev):\n",
    "        rev_link = target_reviews[i].find_all(\"a\")[0]['href']\n",
    "        link = main_link + rev_link\n",
    "        while True:\n",
    "          try:\n",
    "            link_request = Request(link)\n",
    "            link_response = urlopen(link_request, context=context)\n",
    "            link_text = link_response.read()\n",
    "            link_response.close()\n",
    "            if link_response.getcode() == 200:\n",
    "              break\n",
    "          except Exception as inst:\n",
    "            print(inst)\n",
    "        \n",
    "        subsoup = BeautifulSoup(link_text, 'html.parser')\n",
    "        \n",
    "        tmp_review = subsoup.find_all('div', itemprop='reviewBody')\n",
    "        if len(tmp_review) < 1:\n",
    "          continue\n",
    "\n",
    "        full_review = tmp_review[0].get_text()\n",
    "        reviews.append(full_review)\n",
    "        print(f\"    Review no. {i} acquired\")\n",
    "    return reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_book_info(main_link, table_of_books, context, start_entry, end_entry, max_rev):\n",
    "  \"\"\"\n",
    "  The function uses the links from table_of_books to retrieve information\n",
    "  about each book. \n",
    "  \n",
    "  Parameter:\n",
    "  table_of_books: dataframe with link to each book as an entry\n",
    "  context: only necessary for Mac in order to get the right ssl key\n",
    "  no_books: this parameter lets you tune how many of the books from \n",
    "   table_of_books you want information about\n",
    "  max_review: parameter allowing you to decide how many reviews you want per. book\n",
    "  \n",
    "  Return:\n",
    "  Dataframe including:\n",
    "  title: title of the book\n",
    "  rating: goodreads rating of the book\n",
    "  related: list of related works\n",
    "  genre: top genre picked for this book\n",
    "  review: list of raw text of the reviews of the book \n",
    "  \"\"\"\n",
    "  total_df = pd.DataFrame(columns = ['title', 'rating', 'author', 'rating_count', 'related', 'genre'])\n",
    "  for i in range(start_entry, end_entry):\n",
    "    print(f'Starting scraping of entry no. {i}')\n",
    "    targetUrl = main_link + eval(table_of_books['Book link'][i])\n",
    "    while True:\n",
    "      try:\n",
    "          targetRequest = Request(targetUrl)\n",
    "          response = urlopen(targetRequest, context=context)\n",
    "          responseText = response.read()\n",
    "          response.close()\n",
    "          if response.getcode() == 200:\n",
    "            break\n",
    "      except Exception as inst:\n",
    "          print(inst)\n",
    "\n",
    "    soup = BeautifulSoup(responseText, 'html.parser')\n",
    "\n",
    "    target_title = soup.find_all(\"h1\")\n",
    "    book_title = target_title[0].get_text(strip = True)\n",
    "\n",
    "    target_rating = soup.find_all('span', {'itemprop' : 'ratingValue'})\n",
    "    rating = target_rating[0].get_text(strip = True)\n",
    "\n",
    "    target_author = soup.find_all('span', {'itemprop': 'name'})\n",
    "    author = target_author[0].get_text(strip = True)\n",
    "\n",
    "    target_rating_count = soup.find_all('meta', {'itemprop': 'ratingCount'})\n",
    "    rating_count = int(target_rating_count[0].attrs['content'])\n",
    "\n",
    "    target_related = soup.find_all('li', class_='cover' )\n",
    "    related = []\n",
    "    for rel in target_related:\n",
    "        t = rel.find_all(\"a\")\n",
    "        t1 = t[0].find_all('img')[0]['alt']\n",
    "        related.append(t1)\n",
    "\n",
    "    target_genre = soup.find_all('a', class_='actionLinkLite bookPageGenreLink')\n",
    "    if len(target_genre) < 1:\n",
    "      genre = None\n",
    "    else:\n",
    "      genre = target_genre[0].get_text()\n",
    "\n",
    "    target_reviews = soup.find_all('div', class_='reviewHeader uitext stacked')\n",
    "    max_rev = max_rev\n",
    "    reviews = get_reviews(target_reviews, max_rev)\n",
    "\n",
    "    total_df = total_df.append({'title': book_title,\n",
    "                                  'rating': rating,\n",
    "                                  'author' : author,\n",
    "                                  'rating_count' : rating_count,\n",
    "                                  'related': [related],\n",
    "                                  'genre': genre\n",
    "                                }, ignore_index=True)\n",
    "      \n",
    "  return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about each book is scraped and after every 10th gathered book the information is saved as a pickle\n",
    "# This is done in order to overcome if code fails mid-scraping-process. \n",
    "main_link = \"https://www.goodreads.com\"\n",
    "chunks = np.arange(0, 20000, 10)\n",
    "total_df = pd.DataFrame(columns = ['title', 'rating', 'related', 'genre'])\n",
    "for i in range(len(chunks)-1):\n",
    "  book_info = scrape_book_info(main_link, table_of_books, context, chunks[i], chunks[i+1], 15)\n",
    "  total_df = total_df.append(book_info)\n",
    "  total_df.to_pickle('extended_book_info.pcl')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
